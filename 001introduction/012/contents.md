
# 目的関数を定義する (5)

また，推定する値が連続値である回帰問題の場合は次の二乗誤差を使います

$$\\|y - f(x)\\|^2$$

これは，Chainerでは `mean_squared_error` として用意されています。

```
loss = F.mean_squared_error(h, t)
```

損失関数のいくつは `chainer.functions` 内で定義されています。
また，自分で新しい損失関数を定義することもできます。

得られた目的関数 $L(\theta)$ は学習対象のモデルのパラメータ $\theta$
によって値が決まる関数であることに注意してください。
この目的関数は，学習データをうまく分類できるような確率分布に対応するパラメータであれば小さい値をとり，そうでない場合は大きな値をとるような関数です。

これにより，学習という問題を目的関数を最小化する最適化問題に変換することができました。

この目的関数をどのように小さくするかは次章で扱います。

## 課題 (*)

なぜ，確率分布の場合，二乗誤差ではなくクロスエントロピー損失関数を使うのか考えてみてください

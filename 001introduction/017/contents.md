# 3. 目的関数を最適化することで，モデルを学習する (5)

誤差逆伝播法で勾配が求められるので，これを使って目的関数の最適化ができますが，問題が一つあります。
目的関数 $L(\theta)$ の勾配 $v(\theta)$ は，目的関数の定義からデータ全てを調べないと求められません。
しかし，毎回勾配を求めるたびにデータを全て調べるのは計算コストが大きすぎます。
そのため，データ全体を使わずにデータの一部だけを利用し勾配の推定値を求め，
それを利用しパラメータを更新します。これを確率的勾配降下法（SGD）とよびます。

パターゴルフで例えるなら，大体こっちの方向が下ってそうだとわかったら，方向を細かく決めずにさっさと打ってしまう方法です。一回打つまでの時間が少なくてすむため単位時間あたりにより多くの回数打つことができ，真面目に方向を定めるより効率よく下ることができます。

この勾配推定に使うためにサンプリングされた学習データをミニバッチとよび，その個数をミニバッチサイズBとします。
ミニバッチサイズは例えば32〜1024程度の値が利用されます。

# 確率的勾配降下法の利点 (*)

勾配降下法には次の二つの問題があります。
一つ目は最適解ではなく局所解に収束してしまう，つまり本当に一番小さい値ではないがその周辺からみると小さい値に収束してしまう問題です。パターゴルフでいえば途中でくぼみがありそこにはまってしまうことに相当します。
二つ目はプラトーと呼ばれる平坦な領域にはまってしまう問題です。例えば平面のような場所では勾配が非常に小さく本当は平面の先にもっと良い解があったとしても，止まってしまう問題があります。

SGDは勾配の推定値を使って更新していくため，常に変数は推定によるノイズの影響である程度ランダムに動き続けます。
このランダムに動くことによって，局所解やプラトーから抜け出すことができます。
# 
# 
# 

# 

